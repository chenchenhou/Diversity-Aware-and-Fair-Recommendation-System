{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0add9307",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from surprise import Dataset, Reader, SVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from itertools import combinations\n",
    "\n",
    "def load_data(ratings_path: str, users_path: str, movies_path: str):\n",
    "    \"\"\"\n",
    "    Load MovieLens 1M dataset from given file paths.\n",
    "    The .dat files use '::' as separator and have no header row.\n",
    "    Returns Pandas DataFrames: ratings_df, users_df, movies_df.\n",
    "    \"\"\"\n",
    "    # Define column names for each file as per the dataset description\n",
    "    ratings_cols = ['UserID', 'MovieID', 'Rating', 'Timestamp']\n",
    "    users_cols   = ['UserID', 'Gender', 'Age', 'Occupation', 'Zip-code']\n",
    "    movies_cols  = ['MovieID', 'Title', 'Genres']\n",
    "    # Load the ratings data (UserID::MovieID::Rating::Timestamp)\n",
    "    ratings_df = pd.read_csv(ratings_path, sep='::', engine='python',\n",
    "                              names=ratings_cols,\n",
    "                              dtype={'UserID': int, 'MovieID': int, 'Rating': int, 'Timestamp': int})\n",
    "    # Load the users data (UserID::Gender::Age::Occupation::Zip-code)\n",
    "    users_df = pd.read_csv(users_path, sep='::', engine='python',\n",
    "                            names=users_cols,\n",
    "                            dtype={'UserID': int, 'Gender': str, 'Age': int, 'Occupation': int, 'Zip-code': str})\n",
    "    # Load the movies data (MovieID::Title::Genres)\n",
    "    movies_df = pd.read_csv(movies_path, sep='::', engine='python',\n",
    "                             names=movies_cols, encoding='latin1',\n",
    "                             dtype={'MovieID': int, 'Title': str, 'Genres': str})\n",
    "    return ratings_df, users_df, movies_df\n",
    "\n",
    "def preprocess_data(ratings_df: pd.DataFrame, users_df: pd.DataFrame, movies_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Preprocess the MovieLens data:\n",
    "    - Handle missing values (if any) by removing incomplete entries.\n",
    "    - Normalize ratings from 1-5 to 0-1 scale.\n",
    "    - Parse the genre strings into lists of genres for each movie.\n",
    "    Returns the cleaned and modified DataFrames.\n",
    "    \"\"\"\n",
    "    # 1. Remove any missing values\n",
    "    ratings_df = ratings_df.dropna()\n",
    "    users_df   = users_df.dropna()\n",
    "    movies_df  = movies_df.dropna()\n",
    "    # 2. Normalize ratings to [0, 1] range (1 -> 0.0, 5 -> 1.0)\n",
    "    ratings_df['Rating_norm'] = (ratings_df['Rating'] - 1) / 4.0\n",
    "    # 3. Parse genres into a list for each movie (split by '|')\n",
    "    movies_df['Genres'] = movies_df['Genres'].apply(\n",
    "        lambda x: x.split('|') if isinstance(x, str) else [])\n",
    "    return ratings_df, users_df, movies_df\n",
    "\n",
    "def split_data(ratings_df: pd.DataFrame, test_ratio: float = 0.2):\n",
    "    \"\"\"\n",
    "    Split ratings data into training and test sets for evaluation.\n",
    "    Ensures each user has at least one rating in train and one in test (if possible).\n",
    "    Returns (train_df, test_df).\n",
    "    \"\"\"\n",
    "    # Group by user and split each user's ratings\n",
    "    train_list = []\n",
    "    test_list = []\n",
    "    for user_id, group in ratings_df.groupby('UserID'):\n",
    "        # Shuffle the user's ratings to randomize selection for test\n",
    "        user_ratings = group.sample(frac=1.0, random_state=42)\n",
    "        # Determine number of test ratings for this user\n",
    "        test_count = max(1, int(len(user_ratings) * test_ratio))\n",
    "        if len(user_ratings) <= 1:\n",
    "            # If the user has only 1 rating, keep it in training (no test data for this user)\n",
    "            train_ratings = user_ratings\n",
    "            test_ratings = pd.DataFrame(columns=ratings_df.columns)\n",
    "        else:\n",
    "            # Split the user's ratings into test and train portions\n",
    "            test_ratings = user_ratings.iloc[:test_count]\n",
    "            train_ratings = user_ratings.iloc[test_count:]\n",
    "            # Ensure train is not empty (if test_ratio rounds up to all ratings)\n",
    "            if train_ratings.empty:\n",
    "                train_ratings = test_ratings.iloc[:1]\n",
    "                test_ratings = test_ratings.iloc[1:]\n",
    "        train_list.append(train_ratings)\n",
    "        test_list.append(test_ratings)\n",
    "    # Concatenate all users' splits and reset index\n",
    "    train_df = pd.concat(train_list).reset_index(drop=True)\n",
    "    test_df  = pd.concat(test_list).reset_index(drop=True)\n",
    "    return train_df, test_df\n",
    "\n",
    "# ------------------ Model Training ---------------------\n",
    "def train_recommender(train_df: pd.DataFrame):\n",
    "    reader = Reader(rating_scale=(1, 5))\n",
    "    data = Dataset.load_from_df(train_df[['UserID', 'MovieID', 'Rating']], reader)\n",
    "    trainset = data.build_full_trainset()\n",
    "    algo = SVD(random_state=42)\n",
    "    algo.fit(trainset)\n",
    "    # Build item latent factor matrix: raw MovieID -> latent vector\n",
    "    item_factors = {int(trainset.to_raw_iid(iid)): algo.qi[iid] for iid in trainset.all_items()}\n",
    "    return algo, trainset, item_factors\n",
    "\n",
    "# --------------- Initial Recommendations ---------------\n",
    "def generate_candidates(model, trainset, user_id: int, movies_df: pd.DataFrame, candidate_pool_size: int = 10):\n",
    "    all_ids = set(movies_df['MovieID'])\n",
    "    try:\n",
    "        inner_uid = trainset.to_inner_uid(user_id)\n",
    "        seen_raw = {int(trainset.to_raw_iid(i)) for i, _ in trainset.ur[inner_uid]}\n",
    "    except ValueError:\n",
    "        seen_raw = set()\n",
    "    pool = []\n",
    "    for mid in all_ids - seen_raw:\n",
    "        est = model.predict(user_id, mid).est\n",
    "        pool.append((mid, est))\n",
    "    pool.sort(key=lambda x: x[1], reverse=True)\n",
    "    return pool[:candidate_pool_size]\n",
    "\n",
    "# -------- Exhaustive Re-ranking (All Sequences) --------\n",
    "def exhaustive_rerank(pool: list, item_factors: dict, movies_df: pd.DataFrame,\n",
    "                     N: int = 5, w_rel: float = 0.6, w_div: float = 0.2, w_fair: float = 0.2):\n",
    "    \"\"\"\n",
    "    Evaluate all possible combinations of length N from the pool\n",
    "    and return the combination with maximum total utility.\n",
    "    Utility of combination is:\n",
    "      w_rel * sum(rel_i) + w_div * diversity + w_fair * fairness\n",
    "    where diversity = 1 - avg pairwise cosine similarity among items,\n",
    "    fairness = coverage_ratio of genres in the combination.\n",
    "    \"\"\"\n",
    "    all_genres = set(g for genres in movies_df['Genres'] for g in genres)\n",
    "    best_combo, best_score, best_total_rel, best_diversity, best_fair, best_genres = None, -np.inf, -np.inf, -np.inf, -np.inf, 0\n",
    "\n",
    "    # Generate all possible combinations\n",
    "    for combo in combinations(pool, N):\n",
    "        mids = [mid for mid, _ in combo]\n",
    "        ests = [est for _, est in combo]\n",
    "\n",
    "        # Calculate overall relevance\n",
    "        total_rel = sum((est - 1) / 4.0 for est in ests) / N\n",
    "\n",
    "        # Calculating diversity: average pairwise cosine similarity\n",
    "        vectors = [item_factors.get(mid) for mid in mids if mid in item_factors]\n",
    "        if len(vectors) >= 2:\n",
    "            stack = np.vstack(vectors)\n",
    "            sim_matrix = cosine_similarity(stack)\n",
    "            # Get the upper triangular matrix (excluding the diagonal)\n",
    "            upper_tri = sim_matrix[np.triu_indices(len(vectors), k=1)]\n",
    "            avg_sim = np.mean(upper_tri) if len(upper_tri) > 0 else 0\n",
    "            diversity = 1 - avg_sim\n",
    "        else:\n",
    "            diversity = 0.0\n",
    "\n",
    "        # Computational Fairness: Coverage\n",
    "        genres_list = [set(movies_df[movies_df['MovieID'] == mid]['Genres'].iloc[0]) for mid in mids]\n",
    "        covered = set().union(*genres_list)\n",
    "        fair = len(covered) / len(all_genres) if all_genres else 0.0\n",
    "        genres = len(covered)\n",
    "\n",
    "        # Calculating overall utility\n",
    "        total_utility = w_rel * total_rel + w_div * diversity + w_fair * fair\n",
    "\n",
    "        # Update best combination\n",
    "        if total_utility > best_score:\n",
    "            best_score = total_utility\n",
    "            best_combo = combo\n",
    "            best_total_rel = total_rel\n",
    "            best_diversity = diversity\n",
    "            best_fair = fair\n",
    "            best_genres = genres\n",
    "\n",
    "    # If the best combination is found, sort it in descending order of relevance score and return it\n",
    "    if best_combo is not None:\n",
    "        best_seq = [mid for mid, _ in sorted(best_combo, key=lambda x: x[1], reverse=True)]\n",
    "        return best_seq, best_total_rel, best_diversity, best_fair, best_genres\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# ---------------- Evaluation Function ------------------\n",
    "def evaluate_recs(model, trainset, item_factors, test_df: pd.DataFrame, users_df: pd.DataFrame, movies_df: pd.DataFrame,\n",
    "                  N: int = 5, candidate_pool_size: int = 10, w_rel: float = 0.6, w_div: float = 0.2, w_fair: float = 0.2):\n",
    "    precisions, recalls, ilds = [], [], []\n",
    "    count = 0\n",
    "    for user in test_df['UserID'].unique():\n",
    "        print(user)\n",
    "        count = count + 1\n",
    "        if count > 10: # only output first 10 persons\n",
    "          break\n",
    "        pool = generate_candidates(model, trainset, user, movies_df, candidate_pool_size)\n",
    "        best_seq, best_total_rel, best_diversity, best_fair, best_genres = exhaustive_rerank(pool, item_factors, movies_df, N, w_rel, w_div, w_fair)\n",
    "        print(\"Best Sequence:\", best_seq)\n",
    "        print(\"Best Total Relevance:\", best_total_rel)\n",
    "        print(\"Best Diversity:\", best_diversity)\n",
    "        print(\"Best Fairness:\", best_fair)\n",
    "        print(\"Best Genres:\", best_genres)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2978c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load the data\n",
    "ratings_df, users_df, movies_df = load_data('ml-1m/ratings.dat', 'ml-1m/users.dat', 'ml-1m/movies.dat')\n",
    "\n",
    "# 2. Preprocess the data\n",
    "ratings_df, users_df, movies_df = preprocess_data(ratings_df, users_df, movies_df)\n",
    "\n",
    "# 3. Split into training and test sets for evaluation\n",
    "train_df, test_df = split_data(ratings_df, test_ratio=0.2)\n",
    "\n",
    "# 4. Train the recommendation model on the training set\n",
    "model, trainset, item_factors = train_recommender(train_df)\n",
    "\n",
    "# 5. Evaluate with exhaustive reranking\n",
    "evaluate_recs(\n",
    "    model, trainset, item_factors, test_df,\n",
    "    users_df, movies_df, N=5, candidate_pool_size=20,\n",
    "    w_rel = 0.6, w_div = 0.2, w_fair = 0.2\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
